package main

import (
	log "github.com/sirupsen/logrus"
	"os"
	"sync"
)

// A configuration for running MIDA
type MIDAConfig struct {
	// Number of simultaneous browser instances
	NumCrawlers int

	// Number of goroutines storing results data
	NumStorers int

	// If true, TaskLocation is an address for an AMPQ server, and credentials
	// must also be provided (as part of the URI). If false, TaskLocation
	// will be the path to the JSON file we will use to crawl (autogenerated: "MIDA_task.json")
	UseAMPQForTasks bool
	TaskLocation    string

	// Monitoring parameters
	EnableMonitoring bool
	PrometheusPort   int
}

func main() {

	mConfig := MIDAConfig{
		NumCrawlers:      2,
		NumStorers:       DefaultNumStorers,
		UseAMPQForTasks:  false,
		TaskLocation:     "examples/exampleTask.json",
		EnableMonitoring: true,
		PrometheusPort:   DefaultPrometheusPort,
	}

	log.Info(mConfig)

	// Create channels for the pipeline
	monitoringChan := make(chan TaskStats)
	finalResultChan := make(chan FinalMIDAResult)
	rawResultChan := make(chan RawMIDAResult)
	sanitizedTaskChan := make(chan SanitizedMIDATask)
	rawTaskChan := make(chan RawMIDATask)

	// Start goroutine that runs the Prometheus monitoring HTTP server
	if mConfig.EnableMonitoring {
		go RunPrometheusClient(monitoringChan, mConfig.PrometheusPort)
	}

	// Start goroutine(s) that handles crawl results storage
	var storageWG sync.WaitGroup
	storageWG.Add(mConfig.NumStorers)
	for i := 0; i < mConfig.NumStorers; i++ {
		go StoreResults(finalResultChan, mConfig, monitoringChan, &storageWG)
	}

	// Start goroutine that handles crawl results sanitization
	go ValidateResult(rawResultChan, finalResultChan)

	// Start crawler(s) which take sanitized tasks as arguments
	var crawlerWG sync.WaitGroup
	crawlerWG.Add(mConfig.NumCrawlers)
	for i := 0; i < mConfig.NumCrawlers; i++ {
		go CrawlerInstance(sanitizedTaskChan, rawResultChan, mConfig, &crawlerWG)
	}

	// Start goroutine which sanitizes input tasks
	go SanitizeTasks(rawTaskChan, sanitizedTaskChan, mConfig)

	go TaskIntake(rawTaskChan, mConfig)

	// Once all crawlers have completed, we can close the Raw Result Channel
	crawlerWG.Wait()
	close(rawResultChan)

	// We are done when all storage has completed
	storageWG.Wait()

	// Cleanup remaining artifacts
	err := os.RemoveAll(TemporaryDirectory)
	if err != nil {
		log.Warn(err)
	}

	return

}
